{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "np.random.seed(random_state)\n",
    "import random\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(data_path):\n",
    "#     dataset = pd.read_csv(data_path)\n",
    "#     X = dataset.iloc[:, 1:-1].replace(np.nan, 0).values\n",
    "#     y = dataset.iloc[:, -1].values\n",
    "#     label_encoder_x_0 = LabelEncoder()\n",
    "#     X[:, 0] = label_encoder_x_0.fit_transform(X[:,0])\n",
    "#     onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "#     X = onehotencoder.fit_transform(X).toarray()\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#     sc = StandardScaler()\n",
    "#     X_train = sc.fit_transform(X_train)\n",
    "#     X_test = sc.transform(X_test)\n",
    "#     return X_train, y_train, X_test, y_test\n",
    "def preprocess(data_path):\n",
    "    dataset = pd.read_csv(data_path)\n",
    "    X = dataset.iloc[:, 1:-1].replace(np.nan, 0).values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=random_state)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = preprocess('datasets/nba_logreg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name is useless in prediction, player stats are what counts its actually bloating the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_per_layer=[19,6,1]\n",
    "kernel_init='uniform'\n",
    "activation=['relu','relu', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_net import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetwork(units_per_layer, kernel_init, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "epochs = 10\n",
    "mini_batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pgso.gso import GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSuppose you have 3 dims x,y,x\\n\\nFor each dimension you have to specify a range\\nx -> [-1, 1]\\ny -> [-10, 10]\\nz -> [-5, 5]\\nYour bounds array will look like - \\nbounds = [[-1, 1], [-10, 10], [-5, 5]]\\n\\ndims = sum([np.prod(layer['weights'].shape) for layer in classifier.layers])\\nbounds[0][0]\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load pgso/gso\n",
    "from pgso.evaluate import error, evaluate, update_velocity, update_position\n",
    "from multiprocessing import Manager, Process, Lock\n",
    "from pgso.init_particles import create_n_particles\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "def sample_data(X_train, y_train, batch_size, mini_batch_size):\n",
    "    X_train, y_train = shuffle(X_train, y_train) \n",
    "    for i in range(0, mini_batch_size-batch_size+1,batch_size):\n",
    "        yield X_train[i:i+batch_size], y_train[i:i+batch_size]\n",
    "\n",
    "\n",
    "# @jit\n",
    "def PSO_purana(classifier,bounds,maxiter,swarm_init=None, train_data=None):\n",
    "        \n",
    "    num_dimensions=len(swarm_init[0])\n",
    "    err_best_g=-1                   # best error for group\n",
    "    pos_best_g=[]                   # best position for group\n",
    "    num_particles = len(swarm_init)\n",
    "    # establish the swarm\n",
    "    swarm = create_n_particles(num_particles, num_dimensions, swarm_init)\n",
    "    # begin optimization loop\n",
    "    i=0\n",
    "    while i < maxiter:\n",
    "        #print i,err_best_g\n",
    "        # cycle through particles in swarm and evaluate fitness\n",
    "        for j in range(0,num_particles):\n",
    "            swarm[j]['pos_best_i'], swarm[j]['err_best_i']  = evaluate(classifier, swarm[j], train_data)\n",
    "\n",
    "            # determine if current particle is the best (globally)\n",
    "            if swarm[j]['err_i'] < err_best_g or err_best_g == -1:\n",
    "                pos_best_g=list(swarm[j]['position_i'])\n",
    "                err_best_g=float(swarm[j]['err_i'])\n",
    "\n",
    "        # cycle through swarm and update velocities and position\n",
    "        for j in range(0,num_particles):\n",
    "            swarm[j]['velocity_i'] = update_velocity(pos_best_g, swarm[j])\n",
    "            swarm[j]['position_i'] = update_position(bounds, swarm[j])\n",
    "        i+=1\n",
    "\n",
    "    # print final results\n",
    "    #print ('\\n')\n",
    "    #print (pos_best_g,' , ', err_best_g)\n",
    "    return pos_best_g[0], err_best_g\n",
    "\n",
    "# @jit\n",
    "def PSO(classifier, bounds, maxiter, shared_list, return_list, l, num_particles=None, swarm_init=None, pso_train_data=None):\n",
    "\n",
    "    # create minibatches inside PSO        \n",
    "    num_dimensions=len(swarm_init[0])\n",
    "    err_best_g=-1                   # best error for group\n",
    "    pos_best_g=[]                   # best position for group\n",
    "    num_particles = len(swarm_init)\n",
    "    #print('adress of classifier object is: ', id(classifier))\n",
    "    # establish the swarm\n",
    "    # initialize swarm population\n",
    "    #print('len(swarm_init): ', len(swarm_init), 'shape of swarm_init[0]: ', swarm_init[0].shape, '\\n')\n",
    "    swarm = create_n_particles(num_particles, num_dimensions, swarm_init)\n",
    "    # begin optimization loop\n",
    "    i=0\n",
    "    while i < maxiter:\n",
    "        #print i,err_best_g\n",
    "        # cycle through particles in swarm and evaluate fitness\n",
    "        for j in range(0,num_particles):\n",
    "            best_pos, swarm[j]['err_best_i'] = evaluate(classifier, swarm[j], pso_train_data)\n",
    "            swarm[j]['pos_best_i'] = best_pos\n",
    "            # determine if current particle is the best (globally)\n",
    "            if swarm[j]['err_i'] < err_best_g or err_best_g == -1:\n",
    "                pos_best_g=list(swarm[j]['position_i'])\n",
    "                err_best_g=float(swarm[j]['err_i'])\n",
    "        \n",
    "        # update the global best in the manager list after k iterations\n",
    "        # we need to add some mutex lock here\n",
    "        \n",
    "        if i == maxiter//2:\n",
    "            l.acquire()\n",
    "            best_galactic_pos = shared_list[0]\n",
    "            best_galactic_err = shared_list[1]\n",
    "            #print(\"best_galactic_err: \" ,best_galactic_err)\n",
    "            #print(\"best_galactic_pos: \", best_galactic_pos)\n",
    "            if err_best_g < best_galactic_err:\n",
    "                shared_list[1] = err_best_g\n",
    "                #print(err_best_g)\n",
    "                shared_list[0] = pos_best_g\n",
    "            else:\n",
    "                #print(\"changing pos_best_g from\", pos_best_g, \" to \", best_galactic_pos)\n",
    "                #emp_list = []\n",
    "                err_best_g = float(best_galactic_err)\n",
    "                #emp_list.append(best_galactic_pos)\n",
    "                pos_best_g = [best_galactic_pos]\n",
    "            \n",
    "            l.release()\n",
    "        # cycle through swarm and update velocities and position\n",
    "        for j in range(0,num_particles):\n",
    "            swarm[j]['velocity_i'] = update_velocity(pos_best_g, swarm[j])\n",
    "            swarm[j]['position_i'] = update_position(bounds, swarm[j])\n",
    "        i+=1\n",
    "    #print('shape of swarm[0][position_i] is: ', swarm[0]['position_i'].shape)\n",
    "    return_list.append((pos_best_g[0], swarm[:]['position_i']))\n",
    "\n",
    "\n",
    "def start(process_list):\n",
    "    for p in process_list:\n",
    "        p.start()\n",
    "        \n",
    "def stop(process_list):\n",
    "    for p in process_list:\n",
    "        p.join()\n",
    "\n",
    "# @jit\n",
    "def GSO(bounds, num_particles, max_iter, classifier, train_data, epochs, batch_size, mini_batch_size=None):\n",
    "    \"\"\"\n",
    "    Galactic Swarm Optimization:\n",
    "    ----------------------------\n",
    "    A meta-heuristic algorithm insipred by the interplay\n",
    "    of stars, galaxies and superclusters under the influence\n",
    "    of gravity.\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    M: integer \n",
    "    number of galaxies\n",
    "    bounds: \n",
    "    bounds of the search space across each dimension\n",
    "    [lower_bound, upper_bound] * dims\n",
    "    We specify only lower_bound and upper_bound\n",
    "    \n",
    "    \"\"\"\n",
    "    subswarm_bests = []\n",
    "    dims = sum([np.prod(np.array(layer['weights']).shape) for layer in classifier.layers.values()])\n",
    "    print(\"total number of weights -\", dims)\n",
    "    lb = bounds[0]\n",
    "    ub = bounds[1]\n",
    "    # lets set bounds across all dims\n",
    "    bounds = [[lb, ub]]*dims\n",
    "    manager = Manager()\n",
    "    l = Lock()\n",
    "    shared_list = manager.list()\n",
    "    return_list = manager.list()\n",
    "    shared_list = [np.random.uniform(lb, ub, dims), np.inf]\n",
    "    all_processes = []\n",
    "    #pso_batch_size = train_data[0].shape[0]//M\n",
    "    g_best_weights = None\n",
    "    g_best_error = float(\"inf\")\n",
    "    classifiers = [copy.deepcopy(classifier) for _ in range(mini_batch_size//batch_size)]\n",
    "\n",
    "    X_train, y_train = train_data\n",
    "    if not mini_batch_size: mini_batch_size = X_train.shape[0]\n",
    "\n",
    "    print('starting with gso_batch size - {}, mini_batch_size -{} '.format(batch_size, mini_batch_size))\n",
    "    \n",
    "    # create N particles here\n",
    "    swarm_inits = []\n",
    "    for j in range(mini_batch_size//batch_size):\n",
    "        swarm_init = []\n",
    "        for _ in range(num_particles):\n",
    "            swarm_init.append(np.random.uniform(lb, ub, (1, dims)))\n",
    "        swarm_inits.append(swarm_init)\n",
    "\n",
    "    for i in tqdm(range(epochs)):\n",
    "        all_processes = []\n",
    "        sampler = sample_data(X_train, y_train, batch_size, mini_batch_size)\n",
    "        for j in range(mini_batch_size//batch_size):    \n",
    "            pso_train_data = next(sampler)\n",
    "            \n",
    "            #initial= np.random.uniform(-10,10, 2)               # initial starting location [x1,x2...]         \n",
    "            # swarm_init = []\n",
    "            # for _ in range(num_particles):\n",
    "            #     swarm_init.append(np.random.uniform(lb, ub, dims))\n",
    "            \n",
    "            #pso_train_data = (data[0][k*batch_size:(k+1)*pso_batch_size], data[1][k*batch_size:(k+1)*pso_batch_size])\n",
    "            \n",
    "            # print('started batch :',i)\n",
    "            # print('train_data length :', len(pso_train_data))\n",
    "            #print('shape of swarm_inits[j][0]: ', swarm_inits[j][0].shape)\n",
    "            swarm_init = np.array([item.reshape(dims, 1) for item  in swarm_inits[j]])\n",
    "            p = Process(target=PSO, args=(classifiers[j], bounds, max_iter, shared_list, return_list, l, None,swarm_init, pso_train_data))\n",
    "            all_processes.append(p)\n",
    "\n",
    "        start(all_processes)\n",
    "        stop(all_processes)    \n",
    "        #print('elements of return list: ', return_list)\n",
    "        main_swarm_init = [item[0] for item in return_list]\n",
    "        #swarm_inits = [item[1] for item in return_list]\n",
    "        swarm_inits = [main_swarm_init for item in return_list]\n",
    "        best_weights, best_error = PSO_purana(classifier, bounds, max_iter, swarm_init=main_swarm_init, train_data=train_data)\n",
    "\n",
    "        if best_error < g_best_error:\n",
    "            g_best_error = best_error\n",
    "            g_best_weights = best_weights\n",
    "        print('completed epoch {} --------> loss_value: {}'.format(i, best_error)) \n",
    "\n",
    "    prev_index = 0\n",
    "    for layer_id, layer in classifier.layers.items():\n",
    "        num_elements = np.prod(layer['weights'].shape) # we can cache this and pass it down or store it as layer.num_elements\n",
    "        new_weights = g_best_weights[prev_index:prev_index+num_elements]\n",
    "        layer['weights'] = new_weights.reshape(layer['weights'].shape) # changing value midway can cause some error\n",
    "        prev_index += num_elements\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# bounds are across each dimension\n",
    "'''\n",
    "Suppose you have 3 dims x,y,x\n",
    "\n",
    "For each dimension you have to specify a range\n",
    "x -> [-1, 1]\n",
    "y -> [-10, 10]\n",
    "z -> [-5, 5]\n",
    "Your bounds array will look like - \n",
    "bounds = [[-1, 1], [-10, 10], [-5, 5]]\n",
    "\n",
    "dims = sum([np.prod(layer['weights'].shape) for layer in classifier.layers])\n",
    "bounds[0][0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetwork(units_per_layer, kernel_init, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of weights - 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with gso_batch size - 400, mini_batch_size -1400 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:11<03:37, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 0 --------> loss_value: 1.96381770599341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:12<02:29,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 1 --------> loss_value: 1.7984720208106582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:13<01:44,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 2 --------> loss_value: 1.628343478846779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:15<01:16,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 3 --------> loss_value: 1.6331820269069248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:16<00:58,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 4 --------> loss_value: 1.7304198452995936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:18<00:46,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 5 --------> loss_value: 1.6029850162820252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:21<00:39,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 6 --------> loss_value: 1.5410818518353913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:23<00:35,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 7 --------> loss_value: 1.476246273433106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:26<00:32,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 8 --------> loss_value: 1.4606251869338793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:29<00:29,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 9 --------> loss_value: 1.4575358216123513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:33<00:27,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 10 --------> loss_value: 1.4565798526926936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:37<00:27,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 11 --------> loss_value: 1.4318339093183772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:42<00:26,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 12 --------> loss_value: 1.4308935828448255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:47<00:25,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 13 --------> loss_value: 1.4427338449570088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:51<00:21,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 14 --------> loss_value: 1.4411911248633917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:56<00:18,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 15 --------> loss_value: 1.4404624720816204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [01:02<00:14,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 16 --------> loss_value: 1.438633615462752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [01:07<00:10,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 17 --------> loss_value: 1.4416803354947674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [01:13<00:05,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 18 --------> loss_value: 1.4385740084680654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [01:20<00:00,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epoch 19 --------> loss_value: 1.4388945399987656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "units_per_layer=[19,6,1]\n",
    "kernel_init='uniform'\n",
    "activation=['relu','relu', 'sigmoid']\n",
    "bounds = [-10, 10]\n",
    "num_particles = 200\n",
    "max_iter = 25\n",
    "train_data = (X_train, y_train)\n",
    "epochs = 20\n",
    "mini_batch_size = 1400\n",
    "batch_size = 400\n",
    "classifier = GSO(bounds, num_particles, max_iter, classifier, train_data, epochs,batch_size, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6791044776119403"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = []\n",
    "for example  in X_test:\n",
    "    y_pred = classifier.forward_pass(example)\n",
    "    predicted.append(y_pred[0])\n",
    "predicted = np.array(predicted)\n",
    "y_preds = (predicted > 0.5)\n",
    "score = accuracy_score(y_test, y_preds)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
